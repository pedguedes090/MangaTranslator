#!/usr/bin/env python3
"""
Enhanced Manga Translator Module
===============================

A comprehensive translation system for manga/comic text with context-aware AI translation.

ğŸ†• NEW FEATURES:
- Context metadata support for smart pronoun/honorific selection
- Locked output format (translation only - no explanations)  
- Language-specific rules for JA/ZH/KO comics
- SFX and thought bubble specialized handling
- Bubble fitting with character limits
- Line preservation for multi-bubble text

Features:
- AI translation backends (Gemini, DeepInfra Gemma)
- High-quality neural translation (NLLB)
- Context-aware translation with relationship/formality/gender metadata
- Language-specific prompts (Japanese manga, Chinese manhua, Korean manhwa)
- Clean output guarantee (no AI explanations or multiple options)
- Automatic language detection
- Error handling and fallbacks

Author: MangaTranslator Team  
License: MIT
Version: 2.0 (Enhanced Prompt System)
"""

# Translation libraries (kept minimal)
# Removed: GoogleTranslator, pipeline, translators
from api_key_manager import APIKeyManager
pass

# Standard library imports
import requests
import random
import time
import os
import json
import re

# Performance monitoring (optional)
try:
    from performance_monitor import performance_monitor
    PERFORMANCE_MONITORING = True
except ImportError:
    PERFORMANCE_MONITORING = False
    performance_monitor = None

# Performance monitoring
try:
    from performance_monitor import performance_monitor
    PERFORMANCE_MONITORING = True
except ImportError:
    PERFORMANCE_MONITORING = False
    performance_monitor = None


class MangaTranslator:
    """
    Multi-service translator optimized for manga/comic text translation with context awareness.
    
    ğŸ†• OPTIMIZED VERSION 2.1:
    - Translation caching system for performance
    - Batch translation support
    - Improved error handling and fallbacks
    - Smart text preprocessing and post-processing
    - Context metadata support for intelligent translation
    - Clean output guarantee (no AI explanations)
    """
    
    def __init__(self, gemini_api_key=None):
        """
        ğŸš€ ENHANCED TRANSLATOR V3.0 - Initialize with intelligent optimization
        
        Args:
            gemini_api_key (str, optional): Gemini API key for AI translation (deprecated - sá»­ dá»¥ng api_keys.json)
        """
        self.target = "vi"  # Target language: Vietnamese
        
        # ğŸ”§ Initialize smart optimizer
        try:
            from config_optimizer import get_optimizer
            self.optimizer = get_optimizer()
            self.optimization_enabled = True
            print("ğŸ¯ Smart optimization system activated")
        except ImportError:
            print("âš ï¸ Optimizer not available - using default settings")
            self.optimizer = None
            self.optimization_enabled = False
        
        # Enhanced translation cache with intelligent sizing
        cache_size = 10000  # Default
        if self.optimizer:
            cache_size = self.optimizer.config["performance"]["cache_max_size"]
        
        self.translation_cache = {}
        self.cache_hits = 0
        self.total_requests = 0
        self.max_cache_size = cache_size
        
        # Supported source languages mapping
        self.supported_languages = {
            "auto": "Tá»± Ä‘á»™ng nháº­n diá»‡n",
            "ja": "Tiáº¿ng Nháº­t (Manga)",
            "zh": "Tiáº¿ng Trung (Manhua)", 
            "ko": "Tiáº¿ng HÃ n (Manhwa)",
            "en": "Tiáº¿ng Anh"
        }
        
        # Initialize API Key Manager
        self.api_key_manager = APIKeyManager("api_keys.json")
        
        # Backward compatibility: náº¿u cÃ³ gemini_api_key Ä‘Æ°á»£c truyá»n vÃ o
        if gemini_api_key and gemini_api_key.strip():
            print(f"âš ï¸ Äang sá»­ dá»¥ng API key truyá»n vÃ o trá»±c tiáº¿p: {gemini_api_key[:10]}...")
            print("ğŸ’¡ Khuyáº¿n nghá»‹: ThÃªm API key vÃ o file api_keys.json Ä‘á»ƒ sá»­ dá»¥ng tÃ­nh nÄƒng multi-key")
            self.fallback_api_key = gemini_api_key.strip()
        else:
            self.fallback_api_key = None
        
        # Test API key availability (khÃ´ng count usage)
        test_key = self.api_key_manager.get_active_key(count_usage=False)
        if test_key:
            print(f"âœ… Multi-API key system Ä‘Ã£ sáºµn sÃ ng vá»›i {len(self.api_key_manager.config['gemini_api_keys'])} key(s)")
        elif self.fallback_api_key:
            print("âœ… Sá»­ dá»¥ng single API key mode")
        else:
            print("âš ï¸ KhÃ´ng cÃ³ Gemini API key kháº£ dá»¥ - vui lÃ²ng cáº¥u hÃ¬nh api_keys.json")
            
        # Enhanced translation methods mapping with intelligent ordering
        self.translators = {
            "gemini": self._translate_with_gemini,      # Best quality with context
            "deepinfra": self._translate_with_deepinfra, # AI alternative with Gemma model
            "nllb": self._translate_with_nllb,         # High quality, reliable fallback
        }
        
        # Language mapping for NLLB API (FLORES-200 codes)
        self.nllb_language_map = {
            "auto": "eng_Latn",  # Default to English for auto-detect
            "ja": "jpn_Jpan",    # Japanese
            "zh": "zho_Hans",    # Chinese Simplified
            "ko": "kor_Hang",    # Korean
            "en": "eng_Latn",    # English
            "vi": "vie_Latn"     # Vietnamese (target)
        }
        
        # Initialize enhanced common phrases cache
        self._init_common_phrases_cache()
        
        # Performance tracking
        self.performance_stats = {
            "total_translations": 0,
            "successful_translations": 0,
            "cache_hits": 0,
            "method_usage": {},
            "avg_translation_time": 0
        }

    def _init_common_phrases_cache(self):
        """
        ğŸ”¥ CACHE THÃ”NG MINH V3.0 - Khá»Ÿi táº¡o cache vá»›i common phrases Ä‘Æ°á»£c má»Ÿ rá»™ng
        Bao gá»“m cáº£ cÃ¡c cá»¥m tá»« phá»©c táº¡p vÃ  context-aware translations
        """
        self.common_phrases = {
            # ğŸŒ Japanese common phrases (má»Ÿ rá»™ng)
            "ã¯ã„": "ÄÆ°á»£c", "ã„ã„ãˆ": "KhÃ´ng", "ã™ã¿ã¾ã›ã‚“": "Xin lá»—i",
            "ã‚ã‚ŠãŒã¨ã†": "Cáº£m Æ¡n", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™": "Cáº£m Æ¡n",
            "ã“ã‚“ã«ã¡ã¯": "Xin chÃ o", "ãŠã¯ã‚ˆã†": "ChÃ o buá»•i sÃ¡ng",
            "ãŠã‚„ã™ã¿": "ChÃºc ngá»§ ngon", "ãã†ã§ã™ã­": "ÄÃºng váº­y",
            "ã‚ã‹ã‚Šã¾ã—ãŸ": "TÃ´i hiá»ƒu rá»“i", "ãŒã‚“ã°ã£ã¦": "Cá»‘ lÃªn!",
            "ã‚„ã£ãŸ": "LÃ m Ä‘Æ°á»£c rá»“i!", "ã ã‚": "KhÃ´ng Ä‘Æ°á»£c",
            "ã™ã”ã„": "Tuyá»‡t vá»i!", "ã‚„ã°ã„": "Tá»‡ rá»“i!",
            "æœ¬å½“": "Tháº­t sá»±", "å˜˜": "Dá»‘i trÃ¡",
            "å¾…ã£ã¦": "Äá»£i Ä‘Ã£", "åŠ©ã‘ã¦": "GiÃºp tÃ´i",
            
            # Japanese expressions (new)
            "ãã†ã‹": "Ra váº­y", "ãªã‚‹ã»ã©": "Hiá»ƒu rá»“i",
            "å¤§ä¸ˆå¤«": "KhÃ´ng sao Ä‘Ã¢u", "ãŠç–²ã‚Œæ§˜": "Cáº£m Æ¡n báº¡n Ä‘Ã£ váº¥t váº£",
            "é ‘å¼µã‚Œ": "Cá»‘ lÃªn nÃ o!", "ç„¡ç†": "KhÃ´ng thá»ƒ Ä‘Æ°á»£c",
            "å±ãªã„": "Nguy hiá»ƒm!", "è¦‹ã¤ã‘ãŸ": "TÃ¬m tháº¥y rá»“i!",
            "è¡Œã“ã†": "Äi thÃ´i!", "ã‚„ã‚ã¦": "Dá»«ng láº¡i!",
            
            # ğŸ® Chinese common phrases (má»Ÿ rá»™ng)  
            "ä½ å¥½": "Xin chÃ o", "è°¢è°¢": "Cáº£m Æ¡n", "å¯¹ä¸èµ·": "Xin lá»—i",
            "ä¸å®¢æ°”": "KhÃ´ng cÃ³ gÃ¬", "å†è§": "Táº¡m biá»‡t",
            "æ˜¯çš„": "ÄÃºng váº­y", "ä¸æ˜¯": "KhÃ´ng pháº£i",
            "å¥½çš„": "ÄÆ°á»£c rá»“i", "æ²¡é—®é¢˜": "KhÃ´ng váº¥n Ä‘á» gÃ¬",
            "å¤ªå¥½äº†": "QuÃ¡ tuyá»‡t!", "åŠ æ²¹": "Cá»‘ lÃªn!",
            "å°å¿ƒ": "Cáº©n tháº­n", "ç­‰ç­‰": "Äá»£i chÃºt", "æ•‘å‘½": "Cá»©u tÃ´i",
            
            # Chinese expressions (new)
            "èµ°å§": "Äi thÃ´i!", "æ²¡äº‹": "KhÃ´ng sao",
            "çœŸçš„å—": "Tháº­t sá»± Ã ?", "å½“ç„¶": "DÄ© nhiÃªn",
            "ä¸è¦": "Äá»«ng", "å¿«ç‚¹": "Nhanh lÃªn!",
            "æ…¢ç€": "Khoan Ä‘Ã£", "å®Œäº†": "Xong rá»“i",
            "æ€ä¹ˆäº†": "Sao váº­y?", "æ˜ç™½äº†": "Hiá»ƒu rá»“i",
            
            # ğŸ‡°ğŸ‡· Korean common phrases (má»Ÿ rá»™ng)
            "ì•ˆë…•í•˜ì„¸ìš”": "Xin chÃ o", "ê°ì‚¬í•©ë‹ˆë‹¤": "Cáº£m Æ¡n",
            "ì£„ì†¡í•©ë‹ˆë‹¤": "Xin lá»—i", "ë„¤": "VÃ¢ng", "ì•„ë‹ˆìš”": "KhÃ´ng",
            "ê´œì°®ì•„ìš”": "KhÃ´ng sao Ä‘Ã¢u", "ì ê¹ë§Œìš”": "Chá» chÃºt",
            "ë„ì™€ì£¼ì„¸ìš”": "GiÃºp tÃ´i", "í™”ì´íŒ…": "Cá»‘ lÃªn!",
            "ëŒ€ë°•": "Tuyá»‡t vá»i!", "í—": "Háº£?!", "ì™€": "Wow!",
            
            # Korean expressions (new)
            "ê°€ì": "Äi thÃ´i!", "ì•Œê² ì–´": "Hiá»ƒu rá»“i",
            "ì§„ì§œ": "Tháº­t sá»±", "ë§ì•„": "ÄÃºng rá»“i",
            "ì•„ë‹ˆì•¼": "KhÃ´ng pháº£i", "ë¹¨ë¦¬": "Nhanh lÃªn",
            "ì ê¹": "Khoan", "ëë‚¬ì–´": "Xong rá»“i",
            "ë­ì•¼": "CÃ¡i gÃ¬ váº­y?", "ì–´ë–»ê²Œ": "LÃ m sao?",
            
            # ğŸ”Š Sound effects (SFX) - Enhanced
            # Japanese SFX
            "ãƒãƒ³": "BÃ™NG!", "ãƒ‰ãƒ³": "Ráº¦M!", "ã‚­ãƒ©ã‚­ãƒ©": "láº¥p lÃ¡nh",
            "ãƒ‰ã‚­ãƒ‰ã‚­": "thÃ¬nh thá»‹ch", "ãƒ–ãƒ¼ãƒ³": "Vá»ªN!", "ã‚¶ãƒ¼": "Ã o Ã o",
            "ãƒ”ã‚«ãƒ”ã‚«": "lÃ³ng lÃ¡nh", "ã‚¬ã‚¿ã‚¬ã‚¿": "run báº§n báº­t",
            "ãƒšã‚³ãƒšã‚³": "Ä‘Ã³i cá»“n cÃ o", "ãƒ•ãƒ¯ãƒ•ãƒ¯": "má»m má»‹n",
            "ã‚ºã‚ºã‚º": "hÃºp hÃºp", "ãƒ‘ãƒãƒ‘ãƒ": "tÃ©p tÃ©p",
            
            # Chinese SFX  
            "è½°": "BOOM!", "ç °": "Äá»¤C!", "å’”åš“": "Káº®C!",
            "å˜¶": "xÃ¬", "å‘¼": "phÃ¹", "å•ª": "tÃ¡ch",
            "å“—": "Ã o", "å˜­": "bá»¥p", "å’•å’š": "cá»¥c tÃ¡c",
            "æ»´ç­”": "tÃ­ch táº¯c", "å“å½“": "loáº£ng xoáº£ng",
            
            # Korean SFX
            "ì¾…": "Cáº CH!", "ì¿µ": "Ráº¦M!", "íœ˜ìµ": "Vá»ªN!",
            "ë”°ë¥´ë¥´": "lÃ¡ch tÃ¡ch", "ë‘ê·¼ë‘ê·¼": "thÃ¬nh thá»‹ch",
            "í‘": "bÃ¹ng", "ì°°ì¹µ": "cáº¯t", "ì¨": "lanh",
            "ë¶€ì›…": "vÃ¹", "ì¡¸ì¡¸": "rÃ³c rÃ¡ch",
            
            # ğŸ˜Š Emotional expressions
            "ãƒãƒãƒ": "Ha ha ha!", "ãƒ›ãƒ›ãƒ›": "Ho ho ho!",
            "ãˆã¸ã¸": "He he he", "ã†ã†ã†ã†": "Æ¯Æ°Æ°Æ°",
            "å“ˆå“ˆå“ˆ": "Ha ha ha!", "å‘µå‘µ": "He he",
            "í˜¸í˜¸í˜¸": "Ho ho ho", "í•˜í•˜í•˜": "Ha ha ha",
            
            # ğŸ’¥ Action phrases
            "ã‚„ã‚Œã‚„ã‚Œ": "HÃ i tháº­t", "ã¾ãšã„": "Tá»‡ rá»“i",
            "ã—ã¾ã£ãŸ": "Cháº¿t tiá»‡t!", "ã‚ˆã—": "ÄÆ°á»£c rá»“i!",
            "ç³Ÿç³•": "Tá»‡ rá»“i", "å®Œè›‹": "TiÃªu rá»“i",
            "ì•„ì°¨": "á»i", "ì–´ì´êµ¬": "Ã”i trá»i",
            
            # ğŸ¤” Thinking expressions
            "ã†ãƒ¼ã‚“": "á»ªm...", "ãã†ã­ãˆ": "Äá»ƒ xem nÃ o...",
            "å—¯": "á»ªm", "è¿™æ ·å•Š": "Ra váº­y Ã ",
            "ìŒ": "á»ªm", "ê·¸ë ‡êµ¬ë‚˜": "Ra váº­y"
        }
        
        # ğŸ¯ Context-aware phrases (ThÃªm má»›i)
        self.context_phrases = {
            # Formal context
            ("ã§ã™", "formal"): "áº¡", ("ã¾ã™", "formal"): "áº¡",
            ("ìŠµë‹ˆë‹¤", "formal"): "áº¡", ("æ‚¨å¥½", "formal"): "ThÆ°a",
            
            # Casual context  
            ("ã ã‚ˆ", "casual"): "Ä‘áº¥y", ("ã˜ã‚ƒã‚“", "casual"): "mÃ ",
            ("ì•¼", "casual"): "", ("å“¦", "casual"): "á»“",
            
            # Emotional context
            ("ã†ã‚Œã—ã„", "happy"): "vui quÃ¡", ("æ‚²ã—ã„", "sad"): "buá»“n tháº­t",
            ("ê¸°ì˜ë‹¤", "happy"): "vui quÃ¡", ("ìŠ¬í”„ë‹¤", "sad"): "buá»“n tháº­t"
        }
        
        # Add all phrases to main cache
        for original, translated in self.common_phrases.items():
            cache_key = self._get_cache_key(original, "auto", None)
            self.translation_cache[cache_key] = translated
            
        # Add context phrases
        for (original, context_type), translated in self.context_phrases.items():
            cache_key = self._get_cache_key(original, "auto", {"context_type": context_type})
            self.translation_cache[cache_key] = translated

    def _get_cache_key(self, text, source_lang, context):
        """Generate cache key for translation"""
        context_str = ""
        if context:
            context_items = [
                context.get('gender', ''),
                context.get('relationship', ''),
                context.get('formality', ''),
                str(context.get('is_thought', False)),
                str(context.get('is_sfx', False))
            ]
            context_str = "|".join(context_items)
        
        return f"{text.strip().lower()}:{source_lang}:{context_str}"

    def _is_simple_text(self, text):
        """Check if text is simple enough for fast translation"""
        if not text or len(text.strip()) == 0:
            return True
        
        # Simple criteria for fast translation
        text = text.strip()
        return (
            len(text) <= 30 and  # Short text
            not any(char in text for char in ".,!?;:()[]{}") and  # No complex punctuation
            len(text.split()) <= 5  # Few words
        )

    def get_cache_stats(self):
        """Get cache performance statistics"""
        hit_rate = (self.cache_hits / self.total_requests * 100) if self.total_requests > 0 else 0
        return {
            "cache_size": len(self.translation_cache),
            "total_requests": self.total_requests,
            "cache_hits": self.cache_hits,
            "hit_rate": hit_rate
        }

    def clear_cache(self):
        """Clear translation cache"""
        self.translation_cache.clear()
        self.cache_hits = 0
        self.total_requests = 0
        # Re-initialize common phrases
        self._init_common_phrases_cache()

    def get_api_key_status(self):
        """
        Láº¥y tráº¡ng thÃ¡i cá»§a táº¥t cáº£ API key
        
        Returns:
            List[Dict]: Danh sÃ¡ch tráº¡ng thÃ¡i cÃ¡c key
        """
        return self.api_key_manager.get_key_status()
    
    def reset_failed_keys(self):
        """Reset danh sÃ¡ch API key bá»‹ lá»—i"""
        self.api_key_manager.reset_failed_keys()
    
    def add_api_key(self, key: str, name: str, daily_limit: int = 1000):
        """
        ThÃªm API key má»›i
        
        Args:
            key (str): API key
            name (str): TÃªn mÃ´ táº£  
            daily_limit (int): Giá»›i háº¡n sá»­ dá»¥ng hÃ ng ngÃ y
        """
        self.api_key_manager.add_api_key(key, name, daily_limit)

    def translate_batch(self, texts, method="gemini", source_lang="auto", context=None, custom_prompt=None):
        """
        Translate multiple texts in batch for better performance
        
        Args:
            texts (list): List of texts to translate
            method (str): Translation method
            source_lang (str): Source language code
            context (dict, optional): Context metadata
            custom_prompt (str, optional): Custom translation prompt
            
        Returns:
            list: List of translated texts
        """
        if not texts:
            return []
        
        print(f"ğŸ”„ Batch translating {len(texts)} texts using {method}")
        
        # Start batch performance monitoring
        batch_start_time = time.time()
        
        # For Gemini, use optimized batch translation (check availability without counting)
        if method == "gemini" and (self.api_key_manager.get_active_key(count_usage=False) or self.fallback_api_key):
            try:
                # Check cache first for each text ONLY if cache enabled
                cached_results = []
                uncached_texts = []
                uncached_indices = []
                
                if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                    for i, text in enumerate(texts):
                        cache_key = self._get_cache_key(text, source_lang, context)
                        if cache_key in self.translation_cache:
                            cached_results.append((i, self.translation_cache[cache_key]))
                        else:
                            uncached_texts.append(text)
                            uncached_indices.append(i)
                else:
                    # Cache disabled - translate all texts fresh
                    uncached_texts = texts.copy()
                    uncached_indices = list(range(len(texts)))
                    print(f"ğŸ”„ Cache disabled - fresh translation for all {len(texts)} texts")
                
                print(f"ğŸ“Š Cache hits: {len(cached_results)}/{len(texts)}")
                
                # Translate uncached texts in batch
                if uncached_texts:
                    batch_translations = self._translate_with_gemini(uncached_texts, source_lang, context, custom_prompt)
                    
                    # Store batch results in cache ONLY if cache enabled
                    if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                        for text, translation in zip(uncached_texts, batch_translations):
                            cache_key = self._get_cache_key(text, source_lang, context)
                            self.translation_cache[cache_key] = translation
                else:
                    batch_translations = []
                
                # Combine cached and new results
                results = [""] * len(texts)
                for i, cached_result in cached_results:
                    results[i] = cached_result
                for i, (uncached_idx, translation) in enumerate(zip(uncached_indices, batch_translations)):
                    results[uncached_idx] = translation
                
                # Record performance
                batch_duration = time.time() - batch_start_time
                print(f"âœ… Gemini batch completed in {batch_duration:.2f}s")
                
                return results
                
            except Exception as e:
                print(f"âŒ Gemini batch failed: {e}, falling back to individual translations")
                # Fall through to individual translation mode
        
        # Fallback: individual translations (for other methods or if batch fails)
        results = []
        cache_hits = 0
        
        for i, text in enumerate(texts):
            try:
                # Try cache first ONLY if enabled
                translated = None
                if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                    cache_key = self._get_cache_key(text, source_lang, context)
                    if cache_key in self.translation_cache:
                        translated = self.translation_cache[cache_key]
                        cache_hits += 1
                
                if translated is None:
                    # Translate new text
                    translated = self.translate(text, method, source_lang, context, custom_prompt)
                    # Store in cache ONLY if enabled
                    if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                        cache_key = self._get_cache_key(text, source_lang, context)
                        self.translation_cache[cache_key] = translated
                
                results.append(translated)
                
                # Progress update for large batches
                if len(texts) > 10 and (i + 1) % 5 == 0:
                    print(f"ğŸ“Š Progress: {i + 1}/{len(texts)} | Cache hits: {cache_hits}")
                    
            except Exception as e:
                print(f"âŒ Error translating text {i + 1}: {e}")
                results.append(text)  # Fallback to original
        
        # Record batch performance
        batch_duration = time.time() - batch_start_time
        if PERFORMANCE_MONITORING:
            performance_monitor.record_batch_performance(len(texts), batch_duration, cache_hits)
            performance_monitor.record_cache_stats(len(self.translation_cache), self.total_requests, self.cache_hits)
        
        print(f"âœ… Batch completed in {batch_duration:.2f}s. Cache hits: {cache_hits}/{len(texts)}")
        return results

    def smart_translate(self, texts_or_text, method="auto", source_lang="auto", context=None, custom_prompt=None, batch_threshold=3):
        """
        ğŸ§  SMART TRANSLATION V3.0 - AI-powered intelligent translation system
        Automatically optimizes translation approach based on content and system state
        
        Args:
            texts_or_text (str|list): Single text string or list of texts
            method (str): Translation method ("auto" for intelligent selection)
            source_lang (str): Source language code  
            context (dict, optional): Context metadata
            custom_prompt (str, optional): Custom translation prompt
            batch_threshold (int): Minimum number of texts to use batch translation
            
        Returns:
            str|list: Single translated text or list of translated texts
        """
        start_time = time.time()
        
        # ğŸ“Š Prepare context for optimization
        optimization_context = {
            "is_batch": isinstance(texts_or_text, list),
            "text_count": len(texts_or_text) if isinstance(texts_or_text, list) else 1,
            "source_lang": source_lang,
            "quality_priority": method == "gemini" or method == "auto",
            "speed_priority": method == "deepinfra"
        }
        
        if isinstance(texts_or_text, list):
            optimization_context["avg_text_length"] = sum(len(str(t)) for t in texts_or_text) / len(texts_or_text)
        else:
            optimization_context["avg_text_length"] = len(str(texts_or_text))
        
        # ğŸ¯ Get optimized settings if available
        if self.optimization_enabled:
            optimal_settings = self.optimizer.get_optimal_settings(optimization_context)
            
            # Use optimal method if auto-selection
            if method == "auto":
                method = optimal_settings["method"]
                print(f"ğŸ¯ Auto-selected method: {method}")
            
            # Use optimal batch size
            if optimization_context["is_batch"]:
                batch_threshold = optimal_settings["batch_size"]
                print(f"ğŸ¯ Optimal batch threshold: {batch_threshold}")
        
        # Single text input
        if isinstance(texts_or_text, str):
            result = self.translate(texts_or_text, method, source_lang, context, custom_prompt)
            
            # Record performance if optimization enabled
            if self.optimization_enabled:
                duration = time.time() - start_time
                self._record_translation_performance(method, 1, duration, result != texts_or_text)
            
            return result
        
        # List input - intelligent batch processing
        if isinstance(texts_or_text, list):
            text_count = len(texts_or_text)
            
            # ğŸš€ Enhanced batch decision logic
            should_use_batch = (
                text_count >= batch_threshold and 
                method in ["gemini", "deepinfra"] and
                optimization_context["avg_text_length"] < 200  # Don't batch very long texts
            )
            
            if should_use_batch:
                print(f"ğŸš€ Smart batch mode: {text_count} texts with {method}")
                
                # Enhanced context for batch processing
                if context is None:
                    context = {}
                context["is_mega_batch"] = text_count > 20
                context["total_images"] = text_count // 10 + 1  # Estimate
                
                result = self.translate_batch(texts_or_text, method, source_lang, context, custom_prompt)
            else:
                print(f"ğŸ”„ Individual translation mode: {text_count} texts")
                result = []
                cache_hits = 0
                
                for text in texts_or_text:
                    # Check cache first for performance ONLY if enabled
                    translated = None
                    if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                        cache_key = self._get_cache_key(text, source_lang, context)
                        if cache_key in self.translation_cache:
                            translated = self.translation_cache[cache_key]
                            cache_hits += 1
                    
                    if translated is None:
                        translated = self.translate(text, method, source_lang, context, custom_prompt)
                    
                    result.append(translated)
                
                print(f"ğŸ’¾ Individual mode cache hits: {cache_hits}/{text_count}")
            
            # Record batch performance if optimization enabled
            if self.optimization_enabled:
                duration = time.time() - start_time
                self._record_batch_performance(method, text_count, duration, context)
            
            return result
        
        # Invalid input
        raise ValueError("Input must be string or list of strings")
    
    def _record_translation_performance(self, method: str, text_count: int, duration: float, success: bool):
        """Record translation performance for optimizer"""
        if not self.optimization_enabled:
            return
        
        metrics = {
            "method": method,
            "text_count": text_count,
            "duration": duration,
            "texts_per_second": text_count / duration if duration > 0 else 0,
            "success": success,
            "cache_hit_rate": (self.cache_hits / self.total_requests * 100) if self.total_requests > 0 else 0,
            "efficiency_score": self._calculate_efficiency_score(text_count, duration, success)
        }
        
        self.optimizer.record_performance(metrics)
    
    def _record_batch_performance(self, method: str, batch_size: int, duration: float, context: dict):
        """Record batch performance metrics"""
        if not self.optimization_enabled:
            return
        
        cache_hits = context.get("cache_hits", 0) if context else 0
        
        metrics = {
            "method": method,
            "batch_size": batch_size,
            "duration": duration,
            "texts_per_second": batch_size / duration if duration > 0 else 0,
            "cache_hits": cache_hits,
            "cache_hit_rate": (cache_hits / batch_size * 100) if batch_size > 0 else 0,
            "efficiency_score": self._calculate_efficiency_score(batch_size, duration, True),
            "is_mega_batch": batch_size > 20
        }
        
        self.optimizer.record_performance(metrics)
    
    def _calculate_efficiency_score(self, text_count: int, duration: float, success: bool) -> float:
        """Calculate efficiency score (0-100)"""
        if not success:
            return 0
        
        # Base speed score
        speed = text_count / duration if duration > 0 else 0
        speed_score = min(100, speed * 20)  # 5 texts/sec = 100 points
        
        # Cache bonus
        cache_rate = (self.cache_hits / self.total_requests * 100) if self.total_requests > 0 else 0
        cache_bonus = cache_rate * 0.3
        
        return min(100, speed_score + cache_bonus)

    def translate_manga_page(self, text_bubbles, method="gemini", source_lang="auto", context=None, custom_prompt=None):
        """
        Specialized function for translating an entire manga page efficiently
        
        Args:
            text_bubbles (list): List of text bubble contents
            method (str): Translation method
            source_lang (str): Source language code
            context (dict, optional): Context metadata
            custom_prompt (str, optional): Custom translation prompt
            
        Returns:
            list: List of translated text bubbles
        """
        if not text_bubbles:
            return []
        
        print(f"ğŸ“– Translating manga page with {len(text_bubbles)} bubbles")
        
        # Filter out empty bubbles but preserve indices
        non_empty_bubbles = []
        bubble_indices = []
        for i, bubble in enumerate(text_bubbles):
            if bubble and bubble.strip():
                non_empty_bubbles.append(bubble.strip())
                bubble_indices.append(i)
        
        if not non_empty_bubbles:
            return [""] * len(text_bubbles)
        
        # Use batch translation for efficiency
        if method == "gemini" and len(non_empty_bubbles) >= 2:
            translations = self.translate_batch(non_empty_bubbles, method, source_lang, context, custom_prompt)
        else:
            # Fallback to individual translations
            translations = [self.translate(bubble, method, source_lang, context, custom_prompt) for bubble in non_empty_bubbles]
        
        # Map translations back to original bubble positions
        result = [""] * len(text_bubbles)
        for i, translation in enumerate(translations):
            if i < len(bubble_indices):
                result[bubble_indices[i]] = translation
        
        return result

    def translate(self, text, method="google", source_lang="auto", context=None, custom_prompt=None):
        """
        Translate text to Vietnamese using the specified method with context support and caching
        
        Args:
            text (str): Text to translate
            method (str): Translation method ("google", "gemini", "hf", "sogou", "bing")
            source_lang (str): Source language code - "auto", "ja", "zh", "ko", "en"
            context (dict, optional): Context metadata for better translation
            custom_prompt (str, optional): Custom translation style prompt to override defaults
            
        Returns:
            str: Translated text in Vietnamese
        """
        
        # Start performance monitoring
        start_time = time.time() if PERFORMANCE_MONITORING else None
        
        # Update statistics
        self.total_requests += 1
        
        # Early return for empty text
        if not text or not text.strip():
            return ""
        
        # Preprocess text
        processed_text = self._preprocess_text(text)
        text_length = len(processed_text)
        
        # Check cache first ONLY if cache is enabled
        if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
            cache_key = self._get_cache_key(processed_text, source_lang, context)
            if cache_key in self.translation_cache:
                self.cache_hits += 1
                cached_result = self.translation_cache[cache_key]
                print(f"ğŸ’¾ Cache hit: '{processed_text[:30]}...' -> '{cached_result[:30]}...'")
                
                # Record performance for cache hit
                if PERFORMANCE_MONITORING and start_time:
                    performance_monitor.end_translation_timer(start_time, method, text_length, cache_hit=True)
                
                return cached_result
        else:
            print(f"ğŸ”„ Cache disabled - fresh translation for: '{processed_text[:30]}...'")
        
        
        # Validate method and fallback if needed (check availability without counting)
        if method == "gemini" and not (self.api_key_manager.get_active_key(count_usage=False) or self.fallback_api_key):
            print("âš ï¸ Gemini API not available, falling back to DeepInfra Gemma")
            method = "deepinfra"
        elif method == "gemini" and (self.api_key_manager.get_active_key(count_usage=False) or self.fallback_api_key):
            print("ğŸ¤– Using Gemini for context-aware translation")
        
        # Get translator function
        translator_func = self.translators.get(method)
        if not translator_func:
            print(f"âŒ Invalid translation method '{method}', using DeepInfra Gemma")
            translator_func = self._translate_with_deepinfra
            method = "deepinfra"

        try:
            # Perform translation
            if method in ["gemini", "deepinfra"]:
                translated = translator_func(processed_text, source_lang, context, custom_prompt)
            else:
                translated = translator_func(processed_text, source_lang)
            
            # Post-process and validate result
            if translated and translated.strip():
                translated = self._post_process_translation(translated, processed_text)
                
                # Store in cache ONLY if enabled
                if self.optimizer and self.optimizer.config.get("performance", {}).get("cache_enabled", False):
                    cache_key = self._get_cache_key(processed_text, source_lang, context)
                    self.translation_cache[cache_key] = translated
                
                # Record performance for successful translation
                if PERFORMANCE_MONITORING and start_time:
                    performance_monitor.end_translation_timer(start_time, method, text_length, cache_hit=False)
                
                return translated
            else:
                print(f"âš ï¸ Empty translation result for method '{method}', trying fallback")
                # Try NLLB first as it has good translation quality
                if method != "nllb":
                    try:
                        print("ğŸ”„ Trying NLLB API as primary fallback...")
                        fallback_result = self._translate_with_nllb(processed_text, source_lang)
                        
                        if fallback_result and fallback_result.strip() and fallback_result != text:
                            fallback_result = self._post_process_translation(fallback_result, processed_text)
                            self.translation_cache[cache_key] = fallback_result
                            
                            # Record performance for NLLB fallback
                            if PERFORMANCE_MONITORING and start_time:
                                performance_monitor.end_translation_timer(start_time, "nllb_fallback", text_length, cache_hit=False)
                            
                            print(f"âœ… NLLB fallback successful: '{fallback_result}'")
                            return fallback_result
                            
                    except Exception as e:
                        print(f"âŒ NLLB fallback failed: {e}")
                
                # If NLLB also fails, return original text
                print("âš ï¸ NLLB fallback failed, returning original text")
                return text
                
        except Exception as e:
            print(f"âŒ Translation failed with {method}: {e}")
            # Try NLLB first as it has good translation quality
            if method != "nllb":
                try:
                    print("ğŸ”„ Trying NLLB API as primary fallback...")
                    fallback_result = self._translate_with_nllb(processed_text, source_lang)
                    
                    if fallback_result and fallback_result.strip() and fallback_result != text:
                        fallback_result = self._post_process_translation(fallback_result, processed_text)
                        self.translation_cache[cache_key] = fallback_result
                        
                        # Record performance for NLLB fallback
                        if PERFORMANCE_MONITORING and start_time:
                            performance_monitor.end_translation_timer(start_time, "nllb_fallback", text_length, cache_hit=False)
                        
                        print(f"âœ… NLLB fallback successful: '{fallback_result}'")
                        return fallback_result
                        
                except Exception as e2:
                    print(f"âŒ NLLB fallback failed: {e2}")
            
            # If NLLB also fails, return original text
            print("âš ï¸ NLLB exception fallback failed, returning original text")
            return text

    def _post_process_translation(self, translated, original):
        """Post-process translation for better quality"""
        if not translated:
            return original
        
        # Basic cleanup
        translated = translated.strip()
        
        # Remove duplicate punctuation
        translated = re.sub(r'([.!?])\1+', r'\1', translated)
        
        # Fix common spacing issues
        translated = re.sub(r'\s+', ' ', translated)
        
        # Ensure proper Vietnamese punctuation
        translated = translated.replace('...', 'â€¦')
        translated = translated.replace('--', 'â€”')
        
        return translated
    
    def _translate_with_deepinfra(self, text, source_lang="auto", context=None, custom_prompt=None):
        """
        ğŸš€ DEEPINFRA V3.0 - Enhanced AI Translation vá»›i Gemma 3-27B 
        Tá»‘i Æ°u hÃ³a prompt vÃ  xá»­ lÃ½ response cho cháº¥t lÆ°á»£ng dá»‹ch tá»‘i Ä‘a
        """
        try:
            print("ğŸ¤– Using DeepInfra Gemma 3-27B for advanced AI translation...")
            
            # Clean input text
            text = text.strip() if text else ""
            if not text:
                print("âš ï¸ Empty text sent to DeepInfra, skipping")
                return ""
            
            # ğŸ§  Intelligent context analysis
            context_analysis = self._analyze_text_context(text, source_lang)
            
            # Build ultra-advanced translation prompt
            if custom_prompt and custom_prompt.strip():
                translation_instruction = custom_prompt.strip()
            else:
                # Parse context vá»›i smart defaults
                context_info = ""
                if context:
                    context_parts = []
                    if context.get('is_sfx', False):
                        context_parts.append("ğŸ”Š SFX_MODE")
                    if context.get('is_thought', False):
                        context_parts.append("ğŸ’­ THOUGHT_MODE")
                    if context.get('formality') == 'polite':
                        context_parts.append("ğŸ© FORMAL_MODE")
                    elif context.get('formality') == 'casual':
                        context_parts.append("ğŸ˜Š CASUAL_MODE")
                    if context.get('emotion'):
                        context_parts.append(f"ğŸ˜ƒ EMOTION_{context.get('emotion').upper()}")
                    
                    if context_parts:
                        context_info = f" | MODES: {' + '.join(context_parts)}"
                
                # Enhanced language-specific instructions
                lang_instruction = ""
                if source_lang == "ja":
                    lang_instruction = """ğŸŒ JAPANESEâ†’VIETNAMESE EXPERT MODE:
- Keigo detection: ã§ã™/ã¾ã™â†’'áº¡/dáº¡', casualâ†’natural Vietnamese
- Cultural bridge: anime/manga terms to Vietnamese equivalent
- Honorifics: senpaiâ†’'tiá»n bá»‘i', senseiâ†’'tháº§y/sensei'  
- Emotional particles: ã‚ˆ/ã­â†’natural Vietnamese flow"""
                elif source_lang == "zh":
                    lang_instruction = """ğŸ® CHINESEâ†’VIETNAMESE MASTER MODE:
- Classical terms: æ­¦åŠŸâ†’'vÃµ cÃ´ng', å¢ƒç•Œâ†’'cáº£nh giá»›i'
- Hierarchy: æ‚¨â†’'NgÃ i', å¸ˆçˆ¶â†’'sÆ° phá»¥', å‰è¾ˆâ†’'tiá»n bá»‘i'
- Cultural context: manhua style to Vietnamese comic style"""
                elif source_lang == "ko":
                    lang_instruction = """ğŸ‡°ğŸ‡· KOREANâ†’VIETNAMESE PRO MODE:
- Honorific system: -ìš”/-ìŠµë‹ˆë‹¤â†’'áº¡/dáº¡', banmalâ†’casual Vietnamese
- Relationships: í˜•/ëˆ„ë‚˜â†’'anh/chá»‹', ì„ ë°°â†’'tiá»n bá»‘i'
- Modern expressions: manhwa/webtoon style adaptation"""
                else:
                    lang_instruction = "ğŸŒ MULTILINGUALâ†’VIETNAMESE: Smart adaptation to Vietnamese culture and comic style"
                
                # ğŸ¯ MEGA PROMPT V3.0 for DeepInfra
                translation_instruction = f"""ğŸŒ MANGA TRANSLATION SPECIALIST V3.0
Báº¡n lÃ  chuyÃªn gia dá»‹ch manga/comic hÃ ng Ä‘áº§u vá»›i kháº£ nÄƒng AI siÃªu viá»‡t.

ğŸ¯ MISSION: Dá»‹ch "{text}" sang tiáº¿ng Viá»‡t hoÃ n háº£o

{lang_instruction}

ğŸ“‹ CONTEXT ANALYSIS: {context_info}
ğŸ§  AI INSIGHTS: {context_analysis}

ğŸ”¥ TRANSLATION PROTOCOL V3.0:

ğŸ’ CHáº¤T LÆ¯á»¢NG TUYá»†T Äá»I:
- ChÃ­nh xÃ¡c 100% nghÄ©a gá»‘c, khÃ´ng thÃªm bá»›t
- Tá»± nhiÃªn nhÆ° ngÆ°á»i Viá»‡t báº£n Ä‘á»‹a
- Giá»¯ nguyÃªn cáº£m xÃºc vÃ  phong cÃ¡ch gá»‘c
- Bubble-friendly: dá»… Ä‘á»c trong speech bubble

ğŸ—£ï¸ SMART ADDRESSING SYSTEM:
- ThÃ¢n thiáº¿t: tao/mÃ y, anh/em, mÃ¬nh/cáº­u  
- Formal: tÃ´i/anh(chá»‹), con/bá»‘(máº¹), em/anh(chá»‹)
- Respectful: Ä‘á»‡ tá»­/sÆ° phá»¥, nhÃ¢n/ngÃ i

âš¡ SPECIAL HANDLING:
- SFX: Ngáº¯n máº¡nh, Ã¢m thanh Viá»‡t ("Ráº¦M!", "Bá»¤P!")
- Thought: Má»m máº¡i, tá»± nhiÃªn ("...")
- Names: Giá»¯ nguyÃªn tÃªn riÃªng
- Slang: Viá»‡t hÃ³a phÃ¹ há»£p lá»©a tuá»•i

ğŸš« STRICTLY FORBIDDEN:
- Giáº£i thÃ­ch, ghi chÃº, phÃ¢n tÃ­ch
- Multiple versions/options
- "(táº¡m dá»‹ch)" hay labels
- Thay Ä‘á»•i nghÄ©a gá»‘c

âš¡ OUTPUT: CHá»ˆ Báº¢N Dá»ŠCH HOÃ€N Háº¢O DUY NHáº¤T!"""
            
            # Prepare enhanced API request
            url = "https://eien-g4f.onrender.com/api/DeepInfra/chat/completions"
            
            payload = {
                "model": "google/gemma-3-27b-it",  # Latest Gemma model
                "messages": [
                    {
                        "role": "system", 
                        "content": "Báº¡n lÃ  siÃªu chuyÃªn gia dá»‹ch thuáº­t manga/comic vá»›i AI intelligence cá»±c cao. Kháº£ nÄƒng dá»‹ch cá»§a báº¡n ngang ngá»­a cÃ¡c dá»‹ch giáº£ chuyÃªn nghiá»‡p hÃ ng Ä‘áº§u. LuÃ´n tráº£ vá» báº£n dá»‹ch hoÃ n háº£o nháº¥t."
                    },
                    {
                        "role": "user", 
                        "content": translation_instruction
                    }
                ],
                "temperature": 0.1,  # Lower for more consistent results
                "max_tokens": min(300, len(text) * 4),  # Dynamic token calculation
                "top_p": 0.85,  # Optimized for quality
                "repetition_penalty": 1.1,  # Avoid repetition
                "do_sample": True
            }
            
            headers = {
                "Content-Type": "application/json",
                "User-Agent": "MangaTranslator/3.0"
            }
            
            # Send request vá»›i intelligent retry
            for attempt in range(3):  # Up to 3 attempts
                try:
                    response = requests.post(url, headers=headers, json=payload, timeout=25)
                    break
                except requests.Timeout:
                    if attempt < 2:
                        wait_time = (attempt + 1) * 2  # Progressive wait
                        print(f"â° DeepInfra timeout, retrying in {wait_time}s... (attempt {attempt + 1}/3)")
                        time.sleep(wait_time)
                        continue
                    else:
                        raise
            
            # Enhanced response processing
            if response.status_code == 200:
                try:
                    data = response.json()
                    
                    # Extract assistant response vá»›i error handling
                    if "choices" in data and len(data["choices"]) > 0:
                        choice = data["choices"][0]
                        
                        # Handle different response formats
                        if "message" in choice and "content" in choice["message"]:
                            assistant_msg = choice["message"]["content"]
                        elif "text" in choice:
                            assistant_msg = choice["text"]
                        else:
                            print("âŒ Unexpected DeepInfra response format")
                            return self._translate_with_nllb(text, source_lang)
                        
                        # Enhanced cleaning vá»›i AI intelligence
                        translated_text = self._clean_deepinfra_response_v3(assistant_msg, text)
                        
                        if translated_text and translated_text.strip():
                            print(f"âœ… DeepInfra Gemma success: '{translated_text}'")
                            return translated_text
                        else:
                            print("âš ï¸ Empty DeepInfra result after advanced cleaning")
                            return self._translate_with_nllb(text, source_lang)
                    else:
                        print("âŒ No choices in DeepInfra response")
                        return self._translate_with_nllb(text, source_lang)
                        
                except json.JSONDecodeError as e:
                    print(f"âŒ DeepInfra JSON decode error: {e}")
                    return self._translate_with_nllb(text, source_lang)
            else:
                print(f"âŒ DeepInfra API error: {response.status_code}")
                if response.text:
                    print(f"Error details: {response.text[:200]}...")
                return self._translate_with_nllb(text, source_lang)
                
        except Exception as e:
            print(f"âŒ DeepInfra translation failed: {e}")
            # Intelligent fallback to NLLB
            return self._translate_with_nllb(text, source_lang)

    def _clean_deepinfra_response_v3(self, response, original_text):
        """
        ğŸ”§ ENHANCED RESPONSE CLEANING V3.0
        Advanced AI response cleaning vá»›i pattern recognition
        """
        if not response:
            return ""
        
        # Initial cleanup
        cleaned = response.strip()
        
        # Remove common AI prefixes (extended list)
        ai_prefixes = [
            "Báº£n dá»‹ch:", "Dá»‹ch:", "Translation:", "Vietnamese:", "Tiáº¿ng Viá»‡t:",
            "CÃ¢u dá»‹ch:", "Káº¿t quáº£:", "ÄÃ¡p Ã¡n:", "Báº£n dá»‹ch tiáº¿ng Viá»‡t:",
            "Vietnamese translation:", "Tráº£ lá»i:", "CÃ¢u tráº£ lá»i:", "Ná»™i dung:",
            "Output:", "Result:", "Answer:", "Response:", "Translated:",
            "Here's the translation:", "The translation is:", "Dá»‹ch thuáº­t:",
            "ğŸŒ", "âš¡", "ğŸ’", "CHá»ˆ Báº¢N Dá»ŠCH:", "OUTPUT:"
        ]
        
        for prefix in ai_prefixes:
            if cleaned.lower().startswith(prefix.lower()):
                cleaned = cleaned[len(prefix):].strip()
        
        # Remove surrounding quotes intelligently
        quote_pairs = [('"', '"'), ("'", "'"), ('`', '`'), ('"', '"'), (''', ''')]
        for start_quote, end_quote in quote_pairs:
            if cleaned.startswith(start_quote) and cleaned.endswith(end_quote):
                cleaned = cleaned[1:-1].strip()
        
        # Split by explanation markers and take best part
        explanation_markers = [
            "\n\n", "Giáº£i thÃ­ch:", "LÆ°u Ã½:", "ChÃº thÃ­ch:", "Note:",
            "Hoáº·c cÃ³ thá»ƒ", "TÃ¹y ngá»¯ cáº£nh", "Alternatively", "Or:",
            "* ", "â€¢ ", "- ", "[", "(LÆ°u Ã½", "(Giáº£i thÃ­ch", "(Note",
            "CÃ³ thá»ƒ dá»‹ch", "Another option", "PhiÃªn báº£n khÃ¡c"
        ]
        
        best_part = cleaned
        for marker in explanation_markers:
            if marker in cleaned:
                parts = cleaned.split(marker)
                if parts[0].strip() and len(parts[0].strip()) >= 3:
                    best_part = parts[0].strip()
                    break
        
        # Advanced pattern cleaning
        # Remove markdown formatting
        best_part = re.sub(r'\*\*(.*?)\*\*', r'\1', best_part)  # **text** -> text
        best_part = re.sub(r'\*(.*?)\*', r'\1', best_part)      # *text* -> text
        
        # Remove line numbers and bullets
        best_part = re.sub(r'^[\d\.\)\-\*\â€¢]\s*', '', best_part)
        
        # Clean extra whitespace and normalize
        best_part = " ".join(best_part.split())
        
        # Quality check: ensure it's not just repeating original
        if best_part.lower() == original_text.lower():
            return ""
        
        # Length sanity check
        if len(best_part) > len(original_text) * 5:  # Too long, probably includes explanation
            sentences = best_part.split('.')
            if sentences and len(sentences[0]) <= len(original_text) * 3:
                best_part = sentences[0].strip()
        
        # Final cleanup
        best_part = best_part.rstrip('.,!?;:')
        
        return best_part

    def _clean_deepinfra_response(self, response):
        """Clean DeepInfra response to extract only translation"""
        if not response:
            return ""
        
        # Remove common prefixes and explanations
        cleaned = response.strip()
        
        # Remove translation labels
        prefixes_to_remove = [
            "Báº£n dá»‹ch:", "Dá»‹ch:", "Translation:", "Vietnamese:",
            "Tiáº¿ng Viá»‡t:", "CÃ¢u dá»‹ch:", "Káº¿t quáº£:", "ÄÃ¡p Ã¡n:",
            "Báº£n dá»‹ch tiáº¿ng Viá»‡t:", "Vietnamese translation:",
            "Tráº£ lá»i:", "CÃ¢u tráº£ lá»i:", "Ná»™i dung:",
        ]
        
        for prefix in prefixes_to_remove:
            if cleaned.lower().startswith(prefix.lower()):
                cleaned = cleaned[len(prefix):].strip()
        
        # Remove quotes if the entire response is quoted
        if cleaned.startswith('"') and cleaned.endswith('"'):
            cleaned = cleaned[1:-1].strip()
        elif cleaned.startswith("'") and cleaned.endswith("'"):
            cleaned = cleaned[1:-1].strip()
        
        # Split by explanations and take first part
        explanation_markers = [
            "\n\n", "Giáº£i thÃ­ch:", "LÆ°u Ã½:", "ChÃº thÃ­ch:",
            "Hoáº·c cÃ³ thá»ƒ", "TÃ¹y ngá»¯ cáº£nh", "* ", "â€¢ ",
            "[", "(LÆ°u Ã½", "(Giáº£i thÃ­ch"
        ]
        
        for marker in explanation_markers:
            if marker in cleaned:
                parts = cleaned.split(marker)
                if parts[0].strip():
                    cleaned = parts[0].strip()
                    break
        
        # Clean extra whitespace
        cleaned = " ".join(cleaned.split())
        
        return cleaned.rstrip('.,!?;:')

    def _translate_with_nllb(self, text, source_lang="auto"):
        """NLLB API translator as high-quality fallback"""
        try:
            print("ğŸ”„ Using NLLB API as fallback...")
            
            # Map language codes to NLLB format
            source_code = self.nllb_language_map.get(source_lang, "eng_Latn")
            target_code = self.nllb_language_map.get("vi", "vie_Latn")
            
            # Prepare API request
            import urllib.parse
            encoded_text = urllib.parse.quote(text)
            
            url = f"https://winstxnhdw-nllb-api.hf.space/api/v4/translator"
            params = {
                "text": text,
                "source": source_code,
                "target": target_code
            }
            
            response = requests.get(url, params=params, timeout=20)
            
            if response.status_code == 200:
                result = response.json()
                
                # Parse NLLB API response format: {"result": "translated_text"}
                if isinstance(result, dict):
                    if "result" in result:
                        translated = result["result"]
                    elif "text" in result:
                        translated = result["text"]
                    else:
                        # Fallback: try to get first string value
                        translated = next((v for v in result.values() if isinstance(v, str)), None)
                elif isinstance(result, str):
                    translated = result
                else:
                    translated = str(result).strip('"')
                
                if translated and translated.strip() and translated != text:
                    print(f"âœ… NLLB translation successful: '{translated}'")
                    return translated.strip()
                else:
                    print("âš ï¸ NLLB returned empty or same text")
                    return text
            else:
                print(f"âŒ NLLB API error: {response.status_code}")
                return text
                
        except Exception as e:
            print(f"âŒ NLLB API failed: {e}")
            return text

    def _translate_with_gemini(self, text, source_lang="auto", context=None, custom_prompt=None):
        """
        Optimized Gemini translation with improved error handling and smart prompting.
        
        Args:
            text (str): Text to translate or list of texts for batch processing
            source_lang (str): Source language
            context (dict, optional): Context metadata
            custom_prompt (str, optional): Custom prompt override
        """
        # Get API key from manager (COUNT USAGE - thá»±c sá»± dÃ¹ng API)
        current_api_key = self.api_key_manager.get_active_key(count_usage=True)
        if not current_api_key and not self.fallback_api_key:
            raise ValueError("KhÃ´ng cÃ³ Gemini API key kháº£ dá»¥ng - vui lÃ²ng cáº¥u hÃ¬nh api_keys.json")
        
        # Use fallback if no key from manager
        if not current_api_key:
            current_api_key = self.fallback_api_key
        
        # Handle batch translation
        if isinstance(text, list):
            return self._translate_batch_with_gemini(text, source_lang, context, custom_prompt, current_api_key)
        
        # Clean input text
        text = text.strip() if text else ""
        if not text:
            print("âš ï¸ Empty text sent to Gemini, skipping")
            return ""
        
        # Smart prompt selection based on text complexity
        if self._is_simple_text(text):
            prompt = self._get_simple_translation_prompt(text, source_lang, context)
        else:
            prompt = self._get_translation_prompt(text, source_lang, context, custom_prompt)
            
        # Debug logging
        print(f"ğŸ¤– Gemini input: '{text}' | Lang: {source_lang}")
        if context:
            print(f"ğŸ“‹ Context: {context}")
            
        try:
            # Use REST API with optimized configuration
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent"
            
            headers = {
                'Content-Type': 'application/json',
                'X-goog-api-key': current_api_key
            }
            
            data = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": prompt
                            }
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.1,  # Ráº¥t tháº¥p Ä‘á»ƒ tÄƒng tÃ­nh nháº¥t quÃ¡n
                    "maxOutputTokens": min(100, max(20, len(text) * 2)),  # Giá»›i háº¡n cháº·t output
                    "topP": 0.7,
                    "topK": 20,
                    "stopSequences": [
                        "\n\n", "Giáº£i thÃ­ch:", "LÆ°u Ã½:", "Translation:", "Note:"
                    ]
                },
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH", 
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_NONE"
                    }
                ]
            }
            
            # Add timeout and retry logic
            for attempt in range(2):  # Maximum 2 attempts
                try:
                    response = requests.post(url, headers=headers, json=data, timeout=15)
                    break
                except requests.Timeout:
                    if attempt == 0:
                        print("â° Gemini request timeout, retrying...")
                        time.sleep(1)
                        continue
                    else:
                        raise
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    candidate = result['candidates'][0]
                    
                    # Check if response was blocked
                    if candidate.get('finishReason') == 'SAFETY':
                        print("âš ï¸ Gemini response blocked by safety filters, using fallback")
                        return self._translate_with_google(text, source_lang)
                    
                    translated_text = candidate['content']['parts'][0]['text'].strip()
                    
                    # Enhanced cleanup - remove any AI explanations
                    translated_text = self._clean_gemini_response(translated_text)
                    
                    if translated_text:
                        print(f"âœ… Gemini translation: '{translated_text}'")
                        return translated_text
                    else:
                        print("âš ï¸ Empty Gemini result after cleaning")
                        return self._translate_with_google(text, source_lang)
                else:
                    print("âŒ No translation candidates in Gemini response")
                    return self._translate_with_google(text, source_lang)
            else:
                error_msg = response.text if response.text else "Unknown error"
                print(f"âŒ Gemini API error: {response.status_code} - {error_msg}")
                
                # ÄÃ¡nh dáº¥u key bá»‹ lá»—i náº¿u khÃ´ng pháº£i fallback key
                if current_api_key != self.fallback_api_key:
                    self.api_key_manager.mark_key_failed(current_api_key)
                
                return self._translate_with_google(text, source_lang)
            
        except Exception as e:
            print(f"âŒ Gemini translation failed: {e}")
            
            # ÄÃ¡nh dáº¥u key bá»‹ lá»—i náº¿u khÃ´ng pháº£i fallback key
            if current_api_key != self.fallback_api_key:
                self.api_key_manager.mark_key_failed(current_api_key)
            
            # Fallback to Google Translate
            return self._translate_with_google(text, source_lang)
    
    def _translate_batch_with_gemini(self, texts, source_lang="auto", context=None, custom_prompt=None, api_key=None):
        """
        Batch translate multiple texts with Gemini in a single API call.
        Reduces API requests and improves performance.
        
        Args:
            texts (list): List of texts to translate
            source_lang (str): Source language
            context (dict, optional): Context metadata
            custom_prompt (str, optional): Custom prompt override
            api_key (str, optional): API key to use (from manager)
            
        Returns:
            list: List of translated texts
        """
        if not texts:
            return []
        
        # Use provided API key or get from manager (COUNT USAGE khi thá»±c sá»± dÃ¹ng)
        current_api_key = api_key if api_key else self.api_key_manager.get_active_key(count_usage=True)
        if not current_api_key and not self.fallback_api_key:
            print("âŒ KhÃ´ng cÃ³ API key kháº£ dá»¥ng cho batch translation")
            return texts  # Return original texts as fallback
            
        if not current_api_key:
            current_api_key = self.fallback_api_key
        
        # Filter empty texts and track indices
        non_empty_texts = []
        text_indices = []
        for i, text in enumerate(texts):
            if text and text.strip():
                non_empty_texts.append(text.strip())
                text_indices.append(i)
        
        if not non_empty_texts:
            return [""] * len(texts)
        
        print(f"ğŸš€ Gemini batch translation: {len(non_empty_texts)} texts")
        
        # Create batch prompt
        batch_prompt = self._get_batch_translation_prompt(non_empty_texts, source_lang, context, custom_prompt)
        
        try:
            # Use REST API with optimized configuration for batch
            url = "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent"
            
            headers = {
                'Content-Type': 'application/json',
                'X-goog-api-key': current_api_key
            }
            
            # Estimate output tokens needed for batch
            total_input_chars = sum(len(text) for text in non_empty_texts)
            max_output_tokens = min(2048, total_input_chars * 3 + 200)  # Buffer for batch overhead
            
            data = {
                "contents": [
                    {
                        "parts": [
                            {
                                "text": batch_prompt
                            }
                        ]
                    }
                ],
                "generationConfig": {
                    "temperature": 0.2,
                    "maxOutputTokens": max_output_tokens,
                    "topP": 0.8,
                    "topK": 30,
                    "stopSequences": [
                        "\n\n---", "Giáº£i thÃ­ch:", "TUYá»†T Vá»œI!", "DÆ¯á»šI ÄÃ‚Y LÃ€", "Báº¢N Dá»ŠCH"
                    ]
                },
                "safetySettings": [
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH", 
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "threshold": "BLOCK_NONE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "threshold": "BLOCK_NONE"
                    }
                ]
            }
            
            # Send request with timeout
            response = requests.post(url, headers=headers, json=data, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if 'candidates' in result and len(result['candidates']) > 0:
                    candidate = result['candidates'][0]
                    
                    # Check if response was blocked
                    if candidate.get('finishReason') == 'SAFETY':
                        print("âš ï¸ Gemini batch response blocked by safety filters, using fallback")
                        return [self._translate_with_google(text, source_lang) for text in texts]
                    
                    batch_response = candidate['content']['parts'][0]['text'].strip()
                    
                    # Parse batch response
                    translated_texts = self._parse_batch_response(batch_response, len(non_empty_texts))
                    
                    if len(translated_texts) == len(non_empty_texts):
                        # Map back to original list with empties
                        result_list = [""] * len(texts)
                        for i, trans_text in enumerate(translated_texts):
                            result_list[text_indices[i]] = trans_text
                        
                        print(f"âœ… Gemini batch completed: {len(translated_texts)} translations")
                        return result_list
                    else:
                        print(f"âš ï¸ Batch response count mismatch: expected {len(non_empty_texts)}, got {len(translated_texts)}")
                        # Fallback to individual translations
                        return [self._translate_with_google(text, source_lang) for text in texts]
                else:
                    print("âŒ No translation candidates in Gemini batch response")
                    return [self._translate_with_google(text, source_lang) for text in texts]
            else:
                error_msg = response.text if response.text else "Unknown error"
                print(f"âŒ Gemini batch API error: {response.status_code} - {error_msg}")
                
                # ÄÃ¡nh dáº¥u key bá»‹ lá»—i náº¿u khÃ´ng pháº£i fallback key
                if current_api_key != self.fallback_api_key:
                    self.api_key_manager.mark_key_failed(current_api_key)
                
                return [self._translate_with_google(text, source_lang) for text in texts]
                
        except Exception as e:
            print(f"âŒ Gemini batch translation failed: {e}")
            
            # ÄÃ¡nh dáº¥u key bá»‹ lá»—i náº¿u khÃ´ng pháº£i fallback key
            if current_api_key != self.fallback_api_key:
                self.api_key_manager.mark_key_failed(current_api_key)
            
            # Fallback to individual Google translations
            return [self._translate_with_google(text, source_lang) for text in texts]

    def _get_simple_translation_prompt(self, text, source_lang, context=None):
        """Generate simplified prompt for simple/short texts"""
        
        # For very short text, use minimal prompt
        if len(text) <= 10:
            return f'Dá»‹ch "{text}" sang tiáº¿ng Viá»‡t. Chá»‰ tráº£ vá» báº£n dá»‹ch:'
        
        # Build basic context
        context_info = []
        if context:
            if context.get('is_sfx', False):
                context_info.append("Ã¢m thanh")
            if context.get('is_thought', False):
                context_info.append("suy nghÄ©")
            if context.get('formality') == 'polite':
                context_info.append("lá»‹ch sá»±")
        
        context_str = " (" + ", ".join(context_info) + ")" if context_info else ""
        
        return f'Dá»‹ch "{text}" sang tiáº¿ng Viá»‡t{context_str}. Tráº£ vá» báº£n dá»‹ch ngáº¯n gá»n:'

    def _get_batch_translation_prompt(self, texts, source_lang, context=None, custom_prompt=None):
        """
        ğŸš€ SIÃŠU BATCH PROMPT V3.0 - Xá»­ lÃ½ hÃ ng loáº¡t thÃ´ng minh
        ÄÆ°á»£c tá»‘i Æ°u cho viá»‡c dá»‹ch nhiá»u text cÃ¹ng lÃºc vá»›i cháº¥t lÆ°á»£ng cao
        """
        text_count = len(texts)
        
        # Parse context metadata vá»›i intelligent defaults
        if context:
            gender = context.get('gender', 'neutral')
            relationship = context.get('relationship', 'neutral')  
            formality = context.get('formality', 'casual')
            is_thought = context.get('is_thought', False)
            is_sfx = context.get('is_sfx', False)
            is_mega_batch = context.get('is_mega_batch', False)
            total_images = context.get('total_images', 1)
            emotion = context.get('emotion', 'neutral')
            scene_context = context.get('scene_context', '')
        else:
            gender = relationship = emotion = 'neutral'
            formality = 'casual'
            is_thought = is_sfx = is_mega_batch = False
            total_images = 1
            scene_context = ''
        
        # ğŸ§  PhÃ¢n tÃ­ch batch context
        batch_analysis = self._analyze_batch_context(texts, source_lang)
        
        # Build enhanced context info
        context_info = []
        if gender != 'neutral':
            context_info.append(f"GIá»šI TÃNH: {gender}")
        if relationship != 'neutral':
            context_info.append(f"Má»I QUAN Há»†: {relationship}")
        if formality != 'casual':
            context_info.append(f"THá»‚ LOáº I: {formality}")
        if emotion != 'neutral':
            context_info.append(f"Cáº¢M XÃšC: {emotion}")
        if is_thought:
            context_info.append("LOáº I: suy_nghÄ©_ná»™i_tÃ¢m")
        if is_sfx:
            context_info.append("LOáº I: hiá»‡u_á»©ng_Ã¢m_thanh")
        if scene_context:
            context_info.append(f"Bá»I Cáº¢NH: {scene_context}")
        if is_mega_batch:
            context_info.append(f"MEGA_BATCH: {total_images} trang, {text_count} texts")
        if batch_analysis:
            context_info.append(f"AI_PHÃ‚N_TÃCH: {batch_analysis}")
            
        context_str = " | ".join(context_info) if context_info else "Batch chuáº©n"
        
        # Get enhanced language rules
        lang_rules = self._get_enhanced_language_rules(source_lang)
        
        # Create numbered list vá»›i smart formatting
        text_list = ""
        for i, text in enumerate(texts, 1):
            # LÃ m sáº¡ch text Ä‘á»ƒ hiá»ƒn thá»‹ tá»‘t hÆ¡n
            clean_text = text.strip().replace('\n', ' ')
            text_list += f"{i}. {clean_text}\n"
        
        # Intelligent mega batch handling
        if is_mega_batch and text_count > 20:
            mega_instructions = f"""MEGA BATCH ({text_count} texts tá»« {total_images} trang): Giá»¯ nháº¥t quÃ¡n tÃªn nhÃ¢n váº­t, xÆ°ng hÃ´ vÃ  phong cÃ¡ch dá»‹ch."""
        else:
            mega_instructions = f"""BATCH ({text_count} texts): Dá»‹ch cháº¥t lÆ°á»£ng cao, nháº¥t quÃ¡n."""
        
        # Custom prompt override vá»›i enhancements
        if custom_prompt and custom_prompt.strip():
            return f"""Dá»‹ch manga sang tiáº¿ng Viá»‡t. {mega_instructions}

Custom: {custom_prompt.strip()}
Context: {context_str}

{lang_rules}

Texts:
{text_list}

TRáº¢ Vá»€: {text_count} dÃ²ng dá»‹ch tiáº¿ng Viá»‡t thuáº§n."""

        # Standard tá»‘i Æ°u batch prompt
        return f"""Dá»‹ch manga sang tiáº¿ng Viá»‡t. Context: {context_str}

{lang_rules}

QUY Táº®C:
- Dá»‹ch chÃ­nh xÃ¡c, tá»± nhiÃªn nhÆ° ngÆ°á»i Viá»‡t nÃ³i
- XÆ°ng hÃ´ phÃ¹ há»£p vá»›i má»‘i quan há»‡
- SFX: ngáº¯n gá»n, máº¡nh máº½ (VD: "Ráº¦M!", "BOOM!")
- Giá»¯ nháº¥t quÃ¡n trong toÃ n bá»™ batch
- CHá»ˆ tráº£ vá» báº£n dá»‹ch, khÃ´ng giáº£i thÃ­ch

Texts cáº§n dá»‹ch:
{text_list}

TRáº¢ Vá»€: {text_count} dÃ²ng dá»‹ch tiáº¿ng Viá»‡t, má»—i dÃ²ng 1 báº£n dá»‹ch hoÃ n chá»‰nh."""

    def _parse_batch_response(self, response, expected_count):
        """
        Enhanced parsing for mega batch responses with better accuracy
        """
        if not response or not response.strip():
            return [""] * expected_count
        
        # Clean response from unwanted prefixes first
        cleaned_response = response.strip()
        
        # Remove problematic prefixes that AI sometimes adds
        unwanted_prefixes = [
            "TUYá»†T Vá»œI! DÆ¯á»šI ÄÃ‚Y LÃ€ Báº¢N Dá»ŠCH",
            "ÄOáº N TEXT SANG TIáº¾NG VIá»†T, GIá»® NHáº¤T QUÃN TÃŠN NHÃ‚N Váº¬T, XLING HÃ” VÃ€ PHONG CÃCH DICH:",
            "Báº¢N Dá»ŠCH", "ÄOáº N TEXT", "SANG TIáº¾NG VIá»†T",
            "DÆ¯á»šI ÄÃ‚Y LÃ€", "TUYá»†T Vá»œI!", "NHáº¤T QUÃN",
            "TÃŠN NHÃ‚N Váº¬T", "XLING HÃ”", "PHONG CÃCH",
            "DICH:", "Báº£n dá»‹ch:", "Dá»‹ch:"
        ]
        
        for prefix in unwanted_prefixes:
            if cleaned_response.upper().startswith(prefix.upper()):
                cleaned_response = cleaned_response[len(prefix):].strip()
        
        # Remove any remaining header text before numbered items
        lines = cleaned_response.split('\n')
        start_idx = 0
        for i, line in enumerate(lines):
            if re.match(r'^\d+\.\s*', line.strip()):
                start_idx = i
                break
        
        if start_idx > 0:
            cleaned_response = '\n'.join(lines[start_idx:])
        
        # Split by lines and clean
        lines = []
        for line in cleaned_response.strip().split('\n'):
            line = line.strip()
            
            # Skip empty lines and separator lines
            if not line or line.startswith('---') or line.startswith('==='):
                continue
                
            # Remove numbering if present (1. 2. etc)
            line = re.sub(r'^\d+\.\s*', '', line)
            # Remove markdown formatting
            line = re.sub(r'^\*\*|\*\*$', '', line)
            # Remove quotes if present
            line = line.strip('"').strip("'").strip('`')
            
            # Skip meta content and unwanted phrases
            skip_words = [
                'giáº£i thÃ­ch', 'lÆ°u Ã½', 'hoáº·c', 'phÃ¢n tÃ­ch', 'context',
                'tuyá»‡t vá»i', 'dÆ°á»›i Ä‘Ã¢y', 'báº£n dá»‹ch', 'Ä‘oáº¡n text',
                'sang tiáº¿ng viá»‡t', 'nháº¥t quÃ¡n', 'tÃªn nhÃ¢n váº­t',
                'xling hÃ´', 'phong cÃ¡ch', 'dich:'
            ]
            if any(skip_word in line.lower() for skip_word in skip_words):
                continue
                
            if line:  # Only add non-empty lines
                lines.append(line)
        
        # Enhanced logging for mega batch
        if expected_count > 20:  # Mega batch
            print(f"ğŸ” MEGA BATCH parsing: extracted {len(lines)} from {expected_count} expected")
        
        # If we have exactly the expected count, return as is
        if len(lines) == expected_count:
            return lines
        
        # If we have more lines, take the first expected_count
        if len(lines) > expected_count:
            print(f"âš ï¸ Got {len(lines)} lines, taking first {expected_count}")
            return lines[:expected_count]
        
        # If we have fewer lines, try alternative parsing
        if len(lines) < expected_count:
            print(f"âš ï¸ Got {len(lines)} lines, expected {expected_count}")
            
            # Try alternative splitting by common separators
            alternative_lines = []
            for separator in ['\n\n', 'ã€‚\n', 'ï¼\n', 'ï¼Ÿ\n']:
                parts = response.split(separator)
                if len(parts) >= expected_count:
                    alternative_lines = [p.strip() for p in parts if p.strip()][:expected_count]
                    break
            
            if len(alternative_lines) == expected_count:
                print(f"âœ… Alternative parsing succeeded: {len(alternative_lines)} lines")
                return alternative_lines
            
            # Final fallback: pad with empty strings
            print(f"ğŸ”§ Padding {expected_count - len(lines)} missing lines")
            while len(lines) < expected_count:
                lines.append("")
            return lines
        
        return lines

    def _get_translation_prompt(self, text, source_lang, context=None, custom_prompt=None):
        """
        ğŸ”¥ SIÃŠU Tá»I Æ¯U PROMPT V3.0 - Intelligent Context-Aware Translation System
        """
        # If custom prompt provided, enhance it with our framework
        if custom_prompt and custom_prompt.strip():
            base_prompt = custom_prompt.strip()
        else:
            base_prompt = "Dá»‹ch manga/comic sang tiáº¿ng Viá»‡t tá»± nhiÃªn, chuáº©n xÃ¡c vÃ  giá»¯ nguyÃªn tinh tháº§n gá»‘c."
        
        # Parse context metadata with smart defaults
        gender = context.get('gender', 'neutral') if context else 'neutral'
        relationship = context.get('relationship', 'neutral') if context else 'neutral'  
        formality = context.get('formality', 'casual') if context else 'casual'
        bubble_limit = context.get('bubble_limit', None) if context else None
        is_thought = context.get('is_thought', False) if context else False
        is_sfx = context.get('is_sfx', False) if context else False
        scene_context = context.get('scene_context', '') if context else ''
        character_emotion = context.get('emotion', 'neutral') if context else 'neutral'
        
        # ğŸ§  INTELLIGENT CONTEXT ANALYSIS
        context_analysis = self._analyze_text_context(text, source_lang)
        
        # Build enhanced context info with AI insights
        context_info = []
        if gender != 'neutral':
            context_info.append(f"GIá»šI TÃNH: {gender}")
        if relationship != 'neutral':
            context_info.append(f"Má»I QUAN Há»†: {relationship}")
        if formality != 'casual':
            context_info.append(f"THá»‚ LOáº I: {formality}")
        if bubble_limit:
            context_info.append(f"GIá»šI Háº N BUBBLE: {bubble_limit} kÃ½ tá»±")
        if is_thought:
            context_info.append("LOáº I: suy_nghÄ©_ná»™i_tÃ¢m")
        if is_sfx:
            context_info.append("LOáº I: hiá»‡u_á»©ng_Ã¢m_thanh")
        if scene_context:
            context_info.append(f"Bá»I Cáº¢NH: {scene_context}")
        if character_emotion != 'neutral':
            context_info.append(f"Cáº¢M XÃšC: {character_emotion}")
        if context_analysis:
            context_info.append(f"AI_PHÃ‚N_TÃCH: {context_analysis}")
            
        context_str = " | ".join(context_info) if context_info else "Ngá»¯ cáº£nh chuáº©n"
        
        # Get enhanced language-specific rules
        lang_rules = self._get_enhanced_language_rules(source_lang)
        
        # ğŸ¯ PROMPT Tá»I Æ¯U V4.0 - Äáº£m báº£o output sáº¡ch 100%
        return f"""Báº¡n lÃ  chuyÃªn gia dá»‹ch manga chuyÃªn nghiá»‡p. Dá»‹ch "{text}" sang tiáº¿ng Viá»‡t.

NGá»® Cáº¢NH: {context_str}

{lang_rules}

QUY Táº®C Dá»ŠCH:
- Dá»‹ch chÃ­nh xÃ¡c 100% nghÄ©a gá»‘c
- Tá»± nhiÃªn nhÆ° ngÆ°á»i Viá»‡t nÃ³i
- Giá»¯ nguyÃªn cáº£m xÃºc vÃ  phong cÃ¡ch
- SFX: dá»‹ch ngáº¯n gá»n vÃ  máº¡nh máº½ (VD: "Ráº¦M!", "BOOM!")
- XÆ°ng hÃ´ phÃ¹ há»£p: tao/mÃ y (thÃ¢n), anh/em (lá»‹ch sá»±), tÃ´i/báº¡n (trung tÃ­nh)

TUYá»†T Äá»I Cáº¤M:
- Giáº£i thÃ­ch, ghi chÃº, phÃ¢n tÃ­ch
- Tráº£ vá» nhiá»u phiÃªn báº£n
- ThÃªm nhÃ£n nhÆ° "(táº¡m dá»‹ch)"

CHá»ˆ TRáº¢ Vá»€: Báº£n dá»‹ch tiáº¿ng Viá»‡t duy nháº¥t, khÃ´ng cÃ³ gÃ¬ khÃ¡c."""
    def _analyze_text_context(self, text, source_lang):
        """
        ğŸ§  PHÃ‚N TÃCH NGá»® Cáº¢NH THÃ”NG MINH - AI Context Analysis
        Tá»± Ä‘á»™ng nháº­n diá»‡n Ä‘áº·c Ä‘iá»ƒm cá»§a text Ä‘á»ƒ dá»‹ch chÃ­nh xÃ¡c hÆ¡n
        """
        if not text or len(text.strip()) < 2:
            return ""
        
        analysis_parts = []
        text_lower = text.lower()
        
        # ğŸ­ PhÃ¢n tÃ­ch cáº£m xÃºc
        emotions = {
            "vui_váº»": ["ç¬‘", "happy", "ê¸°ì˜", "ã†ã‚Œã—", "å¬‰ã—ã„", "æ¥½ã—ã„", "ãƒãƒ", "í˜¸í˜¸", "haha"],
            "tá»©c_giáº­n": ["æ€’", "angry", "í™”ë‚˜", "ã‚€ã‹ã¤ã", "è…¹ç«‹", "ã‚¯ã‚½", "ç•œç”Ÿ", "ì  ì¥", "damn"],
            "buá»“n": ["æ‚²", "sad", "ìŠ¬í”„", "æ‚²ã—ã„", "æ³£", "crying", "ìš°ëŠ”", "çœ ã‚Œ"],
            "ngáº¡c_nhiÃªn": ["é©š", "surprise", "ë†€ë¼", "ã³ã£ãã‚Š", "ãˆã£", "ã¾ã•ã‹", "í—‰", "ì–´?"],
            "sá»£": ["æ€–", "scared", "ë¬´ì„œ", "æã‚ã—ã„", "ã“ã‚ã„", "æ€–ã„", "ë¬´ì„œì›Œ", "scary"]
        }
        
        for emotion, keywords in emotions.items():
            if any(keyword in text for keyword in keywords):
                analysis_parts.append(f"cáº£m_xÃºc_{emotion}")
                break
        
        # ğŸ—£ï¸ PhÃ¢n tÃ­ch kiá»ƒu lá»i nÃ³i
        speech_patterns = {
            "thá»‘t_ra": ["!", "ï¼", "ã£", "â€¦", "ì–´?", "ãˆã£", "å“"],
            "nghiÃªm_tÃºc": ["ã€‚", "ã ", "ã§ã‚ã‚‹", "ìŠµë‹ˆë‹¤", "ì…ë‹ˆë‹¤"],
            "thÃ¢n_máº­t": ["ã‚ˆ", "ã­", "ã˜ã‚ƒã‚“", "ì•¼", "ì–´", "å‘¢", "å“¦"],
            "kÃªu_gá»i": ["ã•ã‚", "ã‚ˆã—", "ê°€ì", "æ¥", "Come"]
        }
        
        for pattern, markers in speech_patterns.items():
            if any(marker in text for marker in markers):
                analysis_parts.append(f"phong_cÃ¡ch_{pattern}")
                break
        
        # ğŸŒ PhÃ¢n tÃ­ch Ä‘áº·c trÆ°ng ngÃ´n ngá»¯
        if source_lang == "ja":
            if any(keigo in text for keigo in ["ã§ã™", "ã¾ã™", "ã§ã‚ã‚‹", "ã”ã–ã„ã¾ã™"]):
                analysis_parts.append("keigo_formal")
            if any(casual in text for casual in ["ã ã‚ˆ", "ã ã­", "ã˜ã‚ƒã‚“", "ã£ã™"]):
                analysis_parts.append("casual_japanese")
        
        elif source_lang == "ko":
            if any(formal in text for formal in ["ìŠµë‹ˆë‹¤", "ì‹œë‹¤", "í•˜ì‹­ì‹œì˜¤"]):
                analysis_parts.append("korean_formal")
            if any(casual in text for casual in ["ì•¼", "ì–´", "ì§€", "ë‹¤ê³ "]):
                analysis_parts.append("korean_casual")
        
        # ğŸ’­ PhÃ¢n tÃ­ch loáº¡i ná»™i dung
        content_types = {
            "hÃ nh_Ä‘á»™ng": ["æˆ¦", "æˆ¦ã†", "æˆ¦é—˜", "æˆ˜æ–—", "ì‹¸ìš°", "fight", "battle"],
            "lÃ£ng_máº¡n": ["æ„›", "love", "ì‚¬ë‘", "æ‹", "ì¢‹ì•„í•´", "å¥½ã"],
            "hÃ i_hÆ°á»›c": ["ç¬‘", "funny", "ì›ƒê¸´", "é¢ç™½ã„", "ãŠã‹ã—ã„"],
            "bÃ­_áº©n": ["è¬", "mystery", "ì‹ ë¹„", "ä¸æ€è­°", "strange"]
        }
        
        for content_type, keywords in content_types.items():
            if any(keyword in text for keyword in keywords):
                analysis_parts.append(f"thá»ƒ_loáº¡i_{content_type}")
                break
        
        # ğŸ”Š PhÃ¢n tÃ­ch Ã¢m thanh/hiá»‡u á»©ng
        sfx_patterns = [
            "ãƒ‰ãƒ³", "ãƒãƒ³", "ã‚­ãƒ©ã‚­ãƒ©", "ãƒ‰ã‚­ãƒ‰ã‚­",  # Japanese SFX
            "è½°", "ç °", "å’”åš“", "å˜­",              # Chinese SFX  
            "ì¾…", "ì¿µ", "íœ˜ìµ", "ë”°ë¥´ë¥´",           # Korean SFX
            "BANG", "BOOM", "CRASH", "POP"        # English SFX
        ]
        
        if any(sfx in text for sfx in sfx_patterns):
            analysis_parts.append("Ã¢m_thanh_hiá»‡u_á»©ng")
        
        # ğŸ“ PhÃ¢n tÃ­ch Ä‘á»™ dÃ i vÃ  phá»©c táº¡p
        if len(text) > 50:
            analysis_parts.append("text_dÃ i")
        elif len(text) < 10:
            analysis_parts.append("text_ngáº¯n")
        
        return ", ".join(analysis_parts) if analysis_parts else "neutral"

    def _get_enhanced_language_rules(self, source_lang):
        """
        ğŸ”¥ QUY Táº®C NGÃ”N NGá»® SIÃŠU Tá»I Æ¯U V3.0
        Rules Ä‘Æ°á»£c cáº£i tiáº¿n dá»±a trÃªn nghiÃªn cá»©u ngÃ´n ngá»¯ há»c vÃ  kinh nghiá»‡m thá»±c táº¿
        """
        if source_lang == "ja":
            return """JAPANESE:
- ã§ã™/ã¾ã™ â†’ "áº¡/dáº¡" (lá»‹ch sá»±), ã /ã§ã‚ã‚‹ â†’ bÃ¬nh thÆ°á»ng
- SFX: ãƒãƒ³â†’"BÃ™NG!", ãƒ‰ãƒ³â†’"Ráº¦M!", ã‚­ãƒ©ã‚­ãƒ©â†’"láº¥p lÃ¡nh", ãƒ‰ã‚­ãƒ‰ã‚­â†’"thÃ¬nh thá»‹ch"
- ã‚„ã°ã„â†’"Cháº¿t tiá»‡t!", ã™ã”ã„â†’"Tuyá»‡t!", å¤§ä¸ˆå¤«â†’"KhÃ´ng sao", é ‘å¼µã£ã¦â†’"Cá»‘ lÃªn!"
- Onii-chanâ†’"anh trai", Senseiâ†’"tháº§y/cÃ´" """

        elif source_lang == "zh":
            return """CHINESE:
- æ‚¨â†’"NgÃ i", å¸ˆçˆ¶â†’"sÆ° phá»¥", å‰è¾ˆâ†’"tiá»n bá»‘i"
- VÃµlÃ¢m: æ­¦åŠŸâ†’"vÃµ cÃ´ng", å†…åŠŸâ†’"ná»™i cÃ´ng", ä¿®ç‚¼â†’"tu luyá»‡n"
- SFX: è½°â†’"BÃ™MM!", ç °â†’"Äá»¤C!", å’”åš“â†’"Káº®C!"
- èµ°å§â†’"Äi thÃ´i!", æ²¡äº‹â†’"KhÃ´ng sao", åŠ æ²¹â†’"Cá»‘ lÃªn!" """

        elif source_lang == "ko":
            return """KOREAN:
- -ìš”/-ìŠµë‹ˆë‹¤ â†’ "áº¡/dáº¡" (lá»‹ch sá»±), Banmal â†’ bÃ¬nh thÆ°á»ng  
- í˜•/ëˆ„ë‚˜/ì˜¤ë¹ /ì–¸ë‹ˆ â†’ "anh/chá»‹", ì„ ë°°/í›„ë°° â†’ "tiá»n bá»‘i/háº­u bá»‘i"
- SFX: ì¾…â†’"Cáº CH!", ì¿µâ†’"Ráº¦M!", íœ˜ìµâ†’"Vá»ªN!", ë‘ê·¼ë‘ê·¼â†’"thÃ¬nh thá»‹ch"
- ì•„ì´ê³ â†’"Ã”i giá»i!", ì§„ì§œâ†’"Tháº­t sá»± Ã ?", í™”ì´íŒ…â†’"Cá»‘ lÃªn!" """

        else:
            return """GENERAL:
- XÆ°ng hÃ´ phÃ¹ há»£p vá»›i formality vÃ  relationship
- SFX: Ã¢m máº¡nhâ†’"BOOM!/BANG!", Ã¢m nháº¹â†’"lÃ¡ch tÃ¡ch/xÃ o xáº¡c"
- Giá»¯ tÃ­nh cÃ¡ch nhÃ¢n váº­t qua lá»i nÃ³i """

    def _clean_gemini_response(self, response):
        """Enhanced cleaning to remove any AI explanations and return only translation"""
        if not response:
            return ""
            
        # Remove quotes and common prefixes
        cleaned = response.strip().strip('"').strip("'")
        
        # Remove translation labels and prefixes
        prefixes_to_remove = [
            "Báº£n dá»‹ch:", "Dá»‹ch:", "Translation:", "Vietnamese:",
            "Tiáº¿ng Viá»‡t:", "CÃ¢u dá»‹ch:", "Káº¿t quáº£:", "ÄÃ¡p Ã¡n:",
            "Báº£n dá»‹ch tiáº¿ng Viá»‡t:", "Vietnamese translation:",
            "TÃ´i sáº½ dá»‹ch:", "ÄÃ¢y lÃ  báº£n dá»‹ch:", "CÃ¢u tráº£ lá»i:",
            "Dá»‹ch thuáº­t:", "Káº¿t quáº£ dá»‹ch:", "PhiÃªn báº£n:",
            "CHá»ˆ TRáº¢ Vá»€:", "OUTPUT:", "ÄÃ¡p Ã¡n dá»‹ch:",
        ]
        
        for prefix in prefixes_to_remove:
            if cleaned.lower().startswith(prefix.lower()):
                cleaned = cleaned[len(prefix):].strip()
        
        # Split by explanations and take first clean part
        explanation_splits = [
            "\n\n", "\n-", "\n*", "\nâ€¢", " (giáº£i thÃ­ch", " (lÆ°u Ã½",
            " (", "[", "Hoáº·c", "TÃ¹y", "Náº¿u", "* ", "â€¢ ",
            "- ", "Giáº£i thÃ­ch:", "LÆ°u Ã½:", "ChÃº thÃ­ch:",
            "CÃ³ thá»ƒ", "Tuá»³ theo", "TÃ¹y vÃ o", "Ã nghÄ©a:",
            "PhiÃªn báº£n khÃ¡c:", "CÃ¡ch khÃ¡c:", "Hoáº·c cÃ³ thá»ƒ:",
            "\nGiáº£i", "\nLÆ°u", "\nChÃº", "\nHoáº·c"
        ]
        
        for split_pattern in explanation_splits:
            if split_pattern in cleaned:
                parts = cleaned.split(split_pattern, 1)
                if parts[0].strip():
                    cleaned = parts[0].strip()
                    break
        
        # Remove markdown formatting
        cleaned = cleaned.replace("**", "").replace("*", "")
        
        # Clean extra whitespace and newlines
        cleaned = " ".join(cleaned.split())
        
        # Extract core translation if contains AI patterns
        ai_patterns = [
            "cÃ³ thá»ƒ dá»‹ch", "tÃ¹y ngá»¯ cáº£nh", "tuá»³ theo", "hoáº·c lÃ ",
            "má»™t cÃ¡ch khÃ¡c", "phiÃªn báº£n khÃ¡c", "cÃ¡ch khÃ¡c", "nghÄ©a lÃ ",
            "tá»©c lÃ ", "hay lÃ ", "hoáº·c dá»‹ch", "cÃ³ nghÄ©a"
        ]
        
        for pattern in ai_patterns:
            if pattern in cleaned.lower():
                # Extract the first sentence before the pattern
                sentences = cleaned.split('.')
                if sentences and len(sentences[0]) > 3:
                    cleaned = sentences[0].strip()
                    break
        
        # Remove trailing punctuation if excessive
        cleaned = cleaned.rstrip('.,!?;:')
        
        # Final check: if still contains problematic patterns, extract quoted text
        if any(bad in cleaned.lower() for bad in ["dá»‹ch lÃ ", "nghÄ©a gá»‘c", "cÃ³ thá»ƒ hiá»ƒu"]):
            # Look for quoted content
            quotes = ['"', "'", """, """, "'", "'"]
            for quote in quotes:
                if quote in cleaned:
                    parts = cleaned.split(quote)
                    if len(parts) >= 3 and len(parts[1]) > 2:
                        cleaned = parts[1].strip()
                        break
        
        return cleaned

    def _analyze_batch_context(self, texts, source_lang):
        """
        ğŸ” PHÃ‚N TÃCH BATCH CONTEXT - Intelligent Batch Analysis
        PhÃ¢n tÃ­ch tá»•ng thá»ƒ cÃ¡c text trong batch Ä‘á»ƒ dá»‹ch nháº¥t quÃ¡n
        """
        if not texts or len(texts) == 0:
            return ""
        
        analysis_parts = []
        
        # ğŸ“Š Thá»‘ng kÃª batch
        total_chars = sum(len(text) for text in texts)
        avg_length = total_chars / len(texts)
        
        if avg_length > 30:
            analysis_parts.append("text_dÃ i_trung_bÃ¬nh")
        elif avg_length < 10:
            analysis_parts.append("text_ngáº¯n_trung_bÃ¬nh")
        
        # ğŸ­ PhÃ¢n tÃ­ch cáº£m xÃºc chung
        emotion_counts = {"positive": 0, "negative": 0, "neutral": 0}
        
        positive_indicators = ["ç¬‘", "æ¥½ã—ã„", "ê¸°ì˜", "happy", "ì¢‹", "å¥½", "å¬‰ã—ã„", "å¼€å¿ƒ"]
        negative_indicators = ["æ€’", "æ‚²", "angry", "sad", "í™”ë‚˜", "å“­", "æ³£", "ë¬´ì„œ"]
        
        for text in texts:
            if any(pos in text for pos in positive_indicators):
                emotion_counts["positive"] += 1
            elif any(neg in text for neg in negative_indicators):
                emotion_counts["negative"] += 1
            else:
                emotion_counts["neutral"] += 1
        
        dominant_emotion = max(emotion_counts, key=emotion_counts.get)
        if emotion_counts[dominant_emotion] > len(texts) * 0.3:  # 30% threshold
            analysis_parts.append(f"cáº£m_xÃºc_chá»§_Ä‘áº¡o_{dominant_emotion}")
        
        # ğŸ—£ï¸ PhÃ¢n tÃ­ch formality patterns
        formal_count = 0
        casual_count = 0
        
        formal_patterns = ["ã§ã™", "ã¾ã™", "ìŠµë‹ˆë‹¤", "ì‹œë‹¤", "æ‚¨", "ã§ã™"]
        casual_patterns = ["ã ã‚ˆ", "ã˜ã‚ƒã‚“", "ì•¼", "ì–´", "å‘¢", "å“¦"]
        
        for text in texts:
            if any(formal in text for formal in formal_patterns):
                formal_count += 1
            elif any(casual in text for casual in casual_patterns):
                casual_count += 1
        
        if formal_count > casual_count and formal_count > len(texts) * 0.3:
            analysis_parts.append("formality_formal_dominant")
        elif casual_count > formal_count and casual_count > len(texts) * 0.3:
            analysis_parts.append("formality_casual_dominant")
        
        # ğŸ’¥ PhÃ¢n tÃ­ch SFX density
        sfx_count = 0
        sfx_patterns = ["ãƒ‰ãƒ³", "ãƒãƒ³", "è½°", "ç °", "ì¾…", "ì¿µ", "BANG", "BOOM"]
        
        for text in texts:
            if any(sfx in text for sfx in sfx_patterns):
                sfx_count += 1
        
        if sfx_count > len(texts) * 0.2:  # 20% cÃ³ SFX
            analysis_parts.append("batch_nhiá»u_sfx")
        
        # ğŸ“š PhÃ¢n tÃ­ch thá»ƒ loáº¡i content
        action_keywords = ["æˆ¦", "æˆ°", "ì‹¸ìš°", "fight", "battle", "attack"]
        romance_keywords = ["æ„›", "love", "ì‚¬ë‘", "æ‹", "kiss", "heart"]
        comedy_keywords = ["ç¬‘", "funny", "ì›ƒê¸´", "é¢ç™½ã„", "joke"]
        
        action_count = sum(1 for text in texts if any(keyword in text for keyword in action_keywords))
        romance_count = sum(1 for text in texts if any(keyword in text for keyword in romance_keywords))
        comedy_count = sum(1 for text in texts if any(keyword in text for keyword in comedy_keywords))
        
        if action_count > len(texts) * 0.25:
            analysis_parts.append("thá»ƒ_loáº¡i_hÃ nh_Ä‘á»™ng")
        if romance_count > len(texts) * 0.25:
            analysis_parts.append("thá»ƒ_loáº¡i_lÃ£ng_máº¡n")
        if comedy_count > len(texts) * 0.25:
            analysis_parts.append("thá»ƒ_loáº¡i_hÃ i_hÆ°á»›c")
        
        # ğŸ§  Character interaction patterns
        dialogue_indicators = ["!", "?", "â€¦", "ã€‚", "ï¼", "ï¼Ÿ"]
        dialogue_count = sum(1 for text in texts if any(indicator in text for indicator in dialogue_indicators))
        
        if dialogue_count > len(texts) * 0.7:  # 70% cÃ³ dáº¥u hiá»‡u há»™i thoáº¡i
            analysis_parts.append("chá»§_yáº¿u_há»™i_thoáº¡i")
        elif dialogue_count < len(texts) * 0.3:  # <30% há»™i thoáº¡i
            analysis_parts.append("chá»§_yáº¿u_narration")
        
        return ", ".join(analysis_parts) if analysis_parts else "batch_neutral"

    def _preprocess_text(self, text):
        """Enhanced preprocessing for different comic types"""
        if not text:
            return ""
        
        # Basic cleaning
        preprocessed_text = text.replace("ï¼", ".")
        
        # Remove excessive whitespace
        preprocessed_text = " ".join(preprocessed_text.split())
        
        # Clean up common OCR artifacts
        preprocessed_text = preprocessed_text.replace("ï¼ˆ", "(").replace("ï¼‰", ")")
        preprocessed_text = preprocessed_text.replace("ï¼", "!").replace("ï¼Ÿ", "?")
        preprocessed_text = preprocessed_text.replace("ã€‚", ".")
        
        # Fix common OCR errors
        preprocessed_text = preprocessed_text.replace("0", "O")  # Common OCR mistake
        preprocessed_text = preprocessed_text.replace("|", "I")  # Vertical line to I
        
        # Remove extra symbols that might confuse translators
        preprocessed_text = re.sub(r'[â€¢â–ªâ–«â– â–¡â—â—‹â—†â—‡â˜…â˜†]', '', preprocessed_text)
        
        return preprocessed_text.strip()

    def _delay(self):
        """Smart delay to avoid rate limiting"""
        # Shorter delay to improve performance
        delay_time = random.uniform(0.5, 1.5)  # Reduced from 3-5 seconds
        time.sleep(delay_time)
